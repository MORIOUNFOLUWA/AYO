{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "f8ae00f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "6712f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seg1 = pd.read_csv('C:/Users/adeba/Downloads/Project Analysis/T1_FE_SEG 1.csv')\n",
    "df_seg2 = pd.read_csv('C:/Users/adeba/Downloads/Project Analysis/T1_FE_SEG 2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "20cfc7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of features for each segmentation method\n",
    "numeric_cols = df_seg1.columns[0:]  # Exclude the first column (e.g., \"Image type\")\n",
    "df_seg1[numeric_cols] = df_seg1[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "df_seg2[numeric_cols] = df_seg2[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop Missing values \n",
    "df_seg1.dropna(inplace=True)\n",
    "df_seg2.dropna(inplace=True)\n",
    "\n",
    "mean_diff = df_seg1.mean() - df_seg2.mean()\n",
    "print(\"Mean difference for each feature:\")\n",
    "print(mean_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "314d1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the columns to use as features (age, sex, resection type, etc.)\n",
    "features = [\n",
    "    'Elongation', 'Flatness', 'LeastAxisLength', 'MajorAxisLength', \n",
    "    'Maximum2DDiameterColumn', 'Maximum2DDiameterRow', 'Maximum2DDiameterSlice',\n",
    "    'Maximum3DDiameter', 'MeshVolume', 'MinorAxisLength', 'Sphericity', 'SurfaceArea',\n",
    "    'SurfaceVolumeRatio', 'VoxelVolume', '10Percentile', '90Percentile',\n",
    "    'Energy', 'Entropy','InterquartileRange','Kurtosis', 'Maximum', 'MeanAbsoluteDeviation',\n",
    "    'Mean', 'Median', 'Minimum', 'Range','RobustMeanAbsoluteDeviation', 'RootMeanSquared', 'Skewness',\n",
    "    'SmallAreaEmphasis', 'SmallAreaHighGrayLevelEmphasis','SmallAreaLowGrayLevelEmphasis',\n",
    "    'ZoneEntropy','ZonePercentage','ZoneVariance','Busyness','Coarseness','Complexity','Contrast', 'Strength',\n",
    "    'TotalEnergy', 'Uniformity', 'Variance',\t'Autocorrelation',\t'ClusterProminence',\t'ClusterShade',\n",
    "   'ClusterTendency',\t'Contrast',\t'Correlation',\t'DifferenceAverage',\t'DifferenceEntropy',\t'DifferenceVariance',\t\n",
    "    'Id',\t'Idm',\t'Idmn',\t'Idn',\t'Imc1',\t'Imc2',\t'InverseVariance',\t'JointAverage',\t'JointEnergy',\t'JointEntropy',\t\n",
    "    'MCC',\t'MaximumProbability',\t'SumAverage',\t'SumEntropy',\t'SumSquares',\t'DependenceEntropy',\t\n",
    "    'DependenceNonUniformity',\t'DependenceNonUniformityNormalized',\t'DependenceVariance',\t\n",
    "    'GrayLevelNonUniformity',\t'GrayLevelVariance',\t'HighGrayLevelEmphasis',\t'LargeDependenceEmphasis',\t\n",
    "    'LargeDependenceHighGrayLevelEmphasis',\t'LargeDependenceLowGrayLevelEmphasis',\t'LowGrayLevelEmphasis',\t\n",
    "    'SmallDependenceEmphasis',\t'SmallDependenceHighGrayLevelEmphasis',\t'SmallDependenceLowGrayLevelEmphasis',\t\n",
    "    'GrayLevelNonUniformity',\t'GrayLevelNonUniformityNormalized',\t'GrayLevelVariance',\t'HighGrayLevelRunEmphasis',\t\n",
    "    'LongRunEmphasis',\t'LongRunHighGrayLevelEmphasis',\t'LongRunLowGrayLevelEmphasis',\t'LowGrayLevelRunEmphasis',\t'RunEntropy',\t\n",
    "    'RunLengthNonUniformity',\t'RunLengthNonUniformityNormalized',\t'RunPercentage',\t'RunVariance',\t'ShortRunEmphasis',\t\n",
    "    'ShortRunHighGrayLevelEmphasis',\t'ShortRunLowGrayLevelEmphasis',\t'GrayLevelNonUniformity',\t\n",
    "    'GrayLevelNonUniformityNormalized',\t'GrayLevelVariance',\t'HighGrayLevelZoneEmphasis',\t\n",
    "    'LargeAreaEmphasis',\t'LargeAreaHighGrayLevelEmphasis',\t'LargeAreaLowGrayLevelEmphasis',\t\n",
    "    'LowGrayLevelZoneEmphasis',\t'SizeZoneNonUniformity',\t'SizeZoneNonUniformityNormalized'\t\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ffe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target\n",
    "X = df_seg1[features]\n",
    "y = df_seg1['Pathologic grade']  # Replace with the actual target column name\n",
    "\n",
    "# Convert categorical variables to numerical using one-hot encoding\n",
    "#X = pd.get_dummies(X, columns=['Sex', 'Resection_Type', 'Brain_Invasion', 'Other_Features'], drop_first=True)\n",
    "\n",
    "# Create and train the Random Forest classifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Get indices of top 5 important features\n",
    "top_indices = feature_importances.argsort()[::-1][:5]\n",
    "\n",
    "# Display top 5 important features\n",
    "for idx in top_indices:\n",
    "    print(f\"Feature {X.columns[idx]}: Importance = {feature_importances[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get 5 highest correlated features with the target outcome\n",
    "# Extract features and target for seg1\n",
    "X_seg1 = df_seg1.drop(columns=['Pathologic grade'])  # Exclude the target column\n",
    "y_seg1 = df_seg1['Pathologic grade']\n",
    "\n",
    "# Calculate correlation matrix for seg2\n",
    "correlation_seg1 = X_seg1.corrwith(y_seg1)\n",
    "\n",
    "# Get the absolute values of correlations and sort in descending order\n",
    "sorted_correlations_seg1 = correlation_seg1.abs().sort_values(ascending=False)\n",
    "\n",
    "# Get the top 5 highest correlated features\n",
    "top_5_correlated_features_seg1 = sorted_correlations_seg1.head(5)\n",
    "\n",
    "# Display the top 5 correlated features and their correlation values\n",
    "print(\"Top 5 Highest Correlated Features - Seg1:\")\n",
    "print(top_5_correlated_features_seg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8066bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess your data for Seg1\n",
    "df_seg1 = pd.read_csv('C:/Users/adeba/Downloads/Project Analysis/T1_FE_SEG 1.csv')\n",
    "\n",
    "# Define the top 5 correlated features for Segmentation 1\n",
    "top_5_correlated_features_seg1 = ['MinorAxisLength', 'Maximum2DDiameterColumn', 'Strength', 'SurfaceArea', 'Maximum2DDiameterRow']\n",
    "\n",
    "# Get the correlation values for Seg1 (replace with actual code)\n",
    "correlations_seg1 = [0.425, 0.401, 0.395, 0.309, 0.286]\n",
    "\n",
    "# Define the top 5 important features for Segmentation 1\n",
    "top_5_important_features_seg1 = ['Flatness', 'SurfaceArea', 'MinorAxisLength', 'GrayLevelNonUniformity', 'Maximum2DDiameterSlice']\n",
    "\n",
    "# Get feature importances for Seg1 (replace with actual code)\n",
    "importances_seg1 = [0.050, 0.037, 0.035, 0.026, 0.025]\n",
    "\n",
    "# Create subplots for correlated features and feature importances\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))  # Increased the figure size\n",
    "\n",
    "# Define custom colors for the charts\n",
    "importance_colors = ['#e74c3c', '#f39c12', '#8e44ad', '#3498db', '#1abc9c']  # Bright colors\n",
    "correlation_color = '#27ae60'  # Bright green color\n",
    "\n",
    "# Plot important features for Segmentation 1 using barplot with custom colors\n",
    "df_importance_seg1 = pd.DataFrame({'Features': top_5_important_features_seg1, 'Importances': importances_seg1})\n",
    "sns.barplot(data=df_importance_seg1, x='Features', y='Importances', ax=axes[0], palette=importance_colors)\n",
    "axes[0].set_xlabel('Features', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Random Forest Values', fontsize=14, fontweight='bold')\n",
    "axes[0].set_title('Bar Plot for Important Features (Seg1)', fontsize=16, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "for idx, value in enumerate(importances_seg1):\n",
    "    axes[0].text(idx, value + 0.001, f'{value:.3f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot correlated features for Segmentation 1 using pointplot with custom color\n",
    "df_corr_seg1 = pd.DataFrame({'Features': top_5_correlated_features_seg1, 'Correlations': correlations_seg1})\n",
    "sns.pointplot(data=df_corr_seg1, x='Correlations', y='Features', ax=axes[1], join=False, color=correlation_color)\n",
    "axes[1].set_xlabel('Correlation values', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Features', fontsize=14, fontweight='bold')\n",
    "axes[1].set_title('Point Plot for Correlated Features (Seg1)', fontsize=16, fontweight='bold')\n",
    "axes[1].tick_params(axis='y', rotation=0)\n",
    "\n",
    "# Adjust the layout and add space between the charts\n",
    "plt.tight_layout(w_pad=4)  # Increased the space between the charts\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c926e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the columns to use as features (age, sex, resection type, etc.)\n",
    "features = [\n",
    "    'Elongation', 'Flatness', 'LeastAxisLength', 'MajorAxisLength', \n",
    "    'Maximum2DDiameterColumn', 'Maximum2DDiameterRow', 'Maximum2DDiameterSlice',\n",
    "    'Maximum3DDiameter', 'MeshVolume', 'MinorAxisLength', 'Sphericity', 'SurfaceArea',\n",
    "    'SurfaceVolumeRatio', 'VoxelVolume', '10Percentile', '90Percentile',\n",
    "    'Energy', 'Entropy','InterquartileRange','Kurtosis', 'Maximum', 'MeanAbsoluteDeviation',\n",
    "    'Mean', 'Median', 'Minimum', 'Range','RobustMeanAbsoluteDeviation', 'RootMeanSquared', 'Skewness',\n",
    "    'SmallAreaEmphasis', 'SmallAreaHighGrayLevelEmphasis','SmallAreaLowGrayLevelEmphasis',\n",
    "    'ZoneEntropy','ZonePercentage','ZoneVariance','Busyness','Coarseness','Complexity','Contrast', 'Strength',\n",
    "    'TotalEnergy', 'Uniformity', 'Variance',\t'Autocorrelation',\t'ClusterProminence',\t'ClusterShade',\n",
    "   'ClusterTendency',\t'Contrast',\t'Correlation',\t'DifferenceAverage',\t'DifferenceEntropy',\t'DifferenceVariance',\t\n",
    "    'Id',\t'Idm',\t'Idmn',\t'Idn',\t'Imc1',\t'Imc2',\t'InverseVariance',\t'JointAverage',\t'JointEnergy',\t'JointEntropy',\t\n",
    "    'MCC',\t'MaximumProbability',\t'SumAverage',\t'SumEntropy',\t'SumSquares',\t'DependenceEntropy',\t\n",
    "    'DependenceNonUniformity',\t'DependenceNonUniformityNormalized',\t'DependenceVariance',\t\n",
    "    'GrayLevelNonUniformity',\t'GrayLevelVariance',\t'HighGrayLevelEmphasis',\t'LargeDependenceEmphasis',\t\n",
    "    'LargeDependenceHighGrayLevelEmphasis',\t'LargeDependenceLowGrayLevelEmphasis',\t'LowGrayLevelEmphasis',\t\n",
    "    'SmallDependenceEmphasis',\t'SmallDependenceHighGrayLevelEmphasis',\t'SmallDependenceLowGrayLevelEmphasis',\t\n",
    "    'GrayLevelNonUniformity',\t'GrayLevelNonUniformityNormalized',\t'GrayLevelVariance',\t'HighGrayLevelRunEmphasis',\t\n",
    "    'LongRunEmphasis',\t'LongRunHighGrayLevelEmphasis',\t'LongRunLowGrayLevelEmphasis',\t'LowGrayLevelRunEmphasis',\t'RunEntropy',\t\n",
    "    'RunLengthNonUniformity',\t'RunLengthNonUniformityNormalized',\t'RunPercentage',\t'RunVariance',\t'ShortRunEmphasis',\t\n",
    "    'ShortRunHighGrayLevelEmphasis',\t'ShortRunLowGrayLevelEmphasis',\t'GrayLevelNonUniformity',\t\n",
    "    'GrayLevelNonUniformityNormalized',\t'GrayLevelVariance',\t'HighGrayLevelZoneEmphasis',\t\n",
    "    'LargeAreaEmphasis',\t'LargeAreaHighGrayLevelEmphasis',\t'LargeAreaLowGrayLevelEmphasis',\t\n",
    "    'LowGrayLevelZoneEmphasis',\t'SizeZoneNonUniformity',\t'SizeZoneNonUniformityNormalized'\t\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32765228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target\n",
    "X = df_seg2[features]\n",
    "y = df_seg2['Pathologic grade']  # Replace with the actual target column name\n",
    "\n",
    "# Convert categorical variables to numerical using one-hot encoding\n",
    "#X = pd.get_dummies(X, columns=['Sex', 'Resection_Type', 'Brain_Invasion', 'Other_Features'], drop_first=True)\n",
    "\n",
    "# Create and train the Random Forest classifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Get indices of top 5 important features\n",
    "top_indices = feature_importances.argsort()[::-1][:5]\n",
    "\n",
    "# Display top 5 important features\n",
    "for idx in top_indices:\n",
    "    print(f\"Feature {X.columns[idx]}: Importance = {feature_importances[idx]}\")\n",
    "    \n",
    "# Extract features and target for seg2\n",
    "X_seg2 = df_seg2.drop(columns=['Pathologic grade'])  # Exclude the target column\n",
    "y_seg2 = df_seg2['Pathologic grade']\n",
    "\n",
    "# Calculate correlation matrix for seg2\n",
    "correlation_seg2 = X_seg2.corrwith(y_seg2)\n",
    "\n",
    "# Get the absolute values of correlations and sort in descending order\n",
    "sorted_correlations_seg2 = correlation_seg2.abs().sort_values(ascending=False)\n",
    "\n",
    "# Get the top 5 highest correlated features\n",
    "top_5_correlated_features_seg2 = sorted_correlations_seg2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a652058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Define the top 5 correlated features for Segmentation 2\n",
    "top_5_correlated_features_seg2 = ['SurfaceVolumeRatio', 'Coarseness', \n",
    "                    'MeshVolume', 'VoxelVolume', 'DependenceNonUniformity']\n",
    "\n",
    "# Get the correlation values for Seg1 (replace with actual code)\n",
    "correlations_seg2 = [0.311, 0.289, 0.2458, 0.2457, 0.243]\n",
    "\n",
    "# Define the top 5 important features for Segmentation 1\n",
    "top_5_important_features_seg2 = ['LeastAxisLength', 'MeshVolume', 'VoxelVolume', 'Maximum2DDiameterRow', 'Coarseness']\n",
    "\n",
    "# Get feature importances for Seg1 (replace with actual code)\n",
    "importances_seg1 = [0.042, 0.040, 0.023, 0.0218, 0.0216]\n",
    "\n",
    "# Create subplots for correlated features and feature importances\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot important features for Segmentation 1 using barplot with different colors\n",
    "df_importance_seg2 = pd.DataFrame({'Features': top_5_important_features_seg1, 'Importances': importances_seg1})\n",
    "sns.barplot(data=df_importance_seg1, x='Features', y='Importances', ax=axes[0], palette='bright')\n",
    "axes[0].set_xlabel('Features', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Random Forest Values', fontsize=14, fontweight='bold')\n",
    "axes[0].set_title('Bar Plot for Important Features (Grow from Seed)', fontsize=16, fontweight='bold')\n",
    "for item in axes[0].get_xticklabels():\n",
    "    item.set_fontsize(12)\n",
    "    item.set_rotation(45)\n",
    "for idx, value in enumerate(importances_seg1):\n",
    "    axes[0].text(idx, value + 0.001, f'{value:.3f}', ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot correlated features for Segmentation 1 using pointplot\n",
    "df_corr_seg2 = pd.DataFrame({'Features': top_5_correlated_features_seg1, 'Correlations': correlations_seg1})\n",
    "sns.pointplot(data=df_corr_seg1, x='Correlations', y='Features', ax=axes[1], join=False, color='purple')\n",
    "axes[1].set_xlabel('Correlations values', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Features', fontsize=14, fontweight='bold')\n",
    "axes[1].set_title('Point Plot for Correlated Features (Grow from Seed)', fontsize=16, fontweight='bold')\n",
    "for item in axes[1].get_yticklabels():\n",
    "    item.set_fontsize(12)\n",
    "\n",
    "# Adjust the layout and add space between the two charts\n",
    "plt.tight_layout(w_pad=4)  # Increase the space between the charts\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#Perform a linear regression for each segmentation method \n",
    "df_seg1 = pd.read_csv('C:/Users/adeba/Downloads/Project Analysis/T1_FE_SEG 1.csv')\n",
    "\n",
    "# Load and preprocess your data for Seg1\n",
    "df_seg1 = pd.read_csv('C:/Users/adeba/Downloads/Project Analysis/T1_FE_SEG 1.csv')\n",
    "\n",
    "# Select the features for Seg1\n",
    "selected_features_seg1 = [ 'Strength', 'SurfaceArea' , 'Maximum2DDiameterRow', 'GrayLevelNonUniformity',\n",
    "                         'Maximum2DDiameterColumn']\n",
    "\n",
    "# Define the top 5 important features for Segmentation 1\n",
    "top_5_important_features_seg1 = ['Flatness', 'SurfaceArea', 'MinorAxisLength', 'GrayLevelNonUniformity', 'Maximum2DDiameterSlice']\n",
    "\n",
    "# Define the top 5 correlated features for Segmentation 1\n",
    "top_5_correlated_features_seg1 = ['MinorAxisLength', 'Maximum2DDiameterColumn', 'Strength','SurfaceArea', 'Maximum2DDiameterRow']\n",
    "\n",
    "# Extract features and target variable for Seg1\n",
    "X_seg1 = df_seg1[selected_features_seg1]\n",
    "y_seg1 = df_seg1['Pathologic grade']  \n",
    "\n",
    "# Split the data into training and testing sets for Seg1\n",
    "X_train_seg1, X_test_seg1, y_train_seg1, y_test_seg1 = train_test_split(X_seg1, y_seg1, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the linear regression model for Seg1\n",
    "model_seg1 = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data for Seg1\n",
    "model_seg1.fit(X_train_seg1, y_train_seg1)\n",
    "\n",
    "# Make predictions on the test data for Seg1\n",
    "y_pred_seg1 = model_seg1.predict(X_test_seg1)\n",
    "\n",
    "# Evaluate the model for Seg1\n",
    "mse_seg1 = mean_squared_error(y_test_seg1, y_pred_seg1)\n",
    "r2_seg1 = r2_score(y_test_seg1, y_pred_seg1)\n",
    "\n",
    "print(f'Seg1 - Mean Squared Error: {mse_seg1}')\n",
    "print(f'Seg1 - R-squared: {r2_seg1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d34a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Convert predicted values to binary (e.g., threshold at 0.5)\n",
    "y_pred_binary_seg1 = (y_pred_seg1 >= 0.5).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics for Seg2\n",
    "accuracy_seg1 = accuracy_score(y_test_seg1, y_pred_binary_seg1)\n",
    "precision_seg1 = precision_score(y_test_seg1, y_pred_binary_seg1)\n",
    "recall_seg1 = recall_score(y_test_seg1, y_pred_binary_seg1)\n",
    "f1_seg1 = f1_score(y_test_seg1, y_pred_binary_seg1)\n",
    "roc_auc_seg1 = roc_auc_score(y_test_seg1, y_pred_seg1)\n",
    "confusion_matrix_seg1 = confusion_matrix(y_test_seg1, y_pred_binary_seg1)\n",
    "\n",
    "# Print the metrics for Seg2\n",
    "print(f'Seg1 - Accuracy: {accuracy_seg1}')\n",
    "print(f'Seg1 - Precision: {precision_seg1}')\n",
    "print(f'Seg1 - Recall: {recall_seg1}')\n",
    "print(f'Seg1 - F1-Score: {f1_seg1}')\n",
    "print(f'Seg1 - AUC-ROC: {roc_auc_seg1}')\n",
    "print('Seg1 - Confusion Matrix:')\n",
    "print(confusion_matrix_seg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "43909cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load and preprocess your data for Seg2\n",
    "df_seg2 = pd.read_csv('C:/Users/adeba/Downloads/Project Analysis/T1_FE_SEG 2.csv')\n",
    "\n",
    "# Select the features for Seg1\n",
    "selected_features_seg2 = ['SurfaceVolumeRatio','LeastAxisLength',\n",
    "                          'MeshVolume', 'VoxelVolume', 'Maximum2DDiameterRow', 'Coarseness']\n",
    "\n",
    "# Define the top 5 correlated features for Segmentation 2\n",
    "top_5_correlated_features_seg2 = ['SurfaceVolumeRatio', 'MeshVolume', 'VoxelVolume', 'DependenceNonUniformity']\n",
    "\n",
    "# Define the top 5 important features for Segmentation 1\n",
    "top_5_important_features_seg2 = ['LeastAxisLength', 'MeshVolume', 'VoxelVolume', 'Maximum2DDiameterRow', 'Coarseness']\n",
    "\n",
    "\n",
    "# Extract features and target variable for Seg1\n",
    "X_seg2 = df_seg2[selected_features_seg2]\n",
    "y_seg2 = df_seg2['Pathologic grade']  \n",
    "\n",
    "# Split the data into training and testing sets for Seg1\n",
    "X_train_seg2, X_test_seg2, y_train_seg2, y_test_seg2 = train_test_split(X_seg2, y_seg2, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the linear regression model for Seg1\n",
    "model_seg2 = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data for Seg1\n",
    "model_seg2.fit(X_train_seg2, y_train_seg2)\n",
    "\n",
    "# Make predictions on the test data for Seg1\n",
    "y_pred_seg2 = model_seg2.predict(X_test_seg2)\n",
    "\n",
    "# Evaluate the model for Seg1\n",
    "mse_seg2 = mean_squared_error(y_test_seg2, y_pred_seg2)\n",
    "r2_seg2 = r2_score(y_test_seg2, y_pred_seg2)\n",
    "\n",
    "print(f'Seg2 - Mean Squared Error: {mse_seg2}')\n",
    "print(f'Seg2 - R-squared: {r2_seg2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "e8d9d036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seg2 - Accuracy: 0.6896551724137931\n",
      "Seg2 - Precision: 0.7\n",
      "Seg2 - Recall: 0.5384615384615384\n",
      "Seg2 - F1-Score: 0.608695652173913\n",
      "Seg2 - AUC-ROC: 0.7307692307692307\n",
      "Seg2 - Confusion Matrix:\n",
      "[[13  3]\n",
      " [ 6  7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Convert predicted values to binary (e.g., threshold at 0.5)\n",
    "y_pred_binary_seg2 = (y_pred_seg2 >= 0.5).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics for Seg2\n",
    "accuracy_seg2 = accuracy_score(y_test_seg2, y_pred_binary_seg2)\n",
    "precision_seg2 = precision_score(y_test_seg2, y_pred_binary_seg2)\n",
    "recall_seg2 = recall_score(y_test_seg2, y_pred_binary_seg2)\n",
    "f1_seg2 = f1_score(y_test_seg2, y_pred_binary_seg2)\n",
    "roc_auc_seg2 = roc_auc_score(y_test_seg2, y_pred_seg2)\n",
    "confusion_matrix_seg2 = confusion_matrix(y_test_seg2, y_pred_binary_seg2)\n",
    "\n",
    "# Print the metrics for Seg2\n",
    "print(f'Seg2 - Accuracy: {accuracy_seg2}')\n",
    "print(f'Seg2 - Precision: {precision_seg2}')\n",
    "print(f'Seg2 - Recall: {recall_seg2}')\n",
    "print(f'Seg2 - F1-Score: {f1_seg2}')\n",
    "print(f'Seg2 - AUC-ROC: {roc_auc_seg2}')\n",
    "print('Seg2 - Confusion Matrix:')\n",
    "print(confusion_matrix_seg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f276264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Haudorff Distance values from 1st Segmentation Method\n",
    "haus_data_thres = pd.read_csv(\"C:/Users/adeba/Downloads/Project Analysis/Seg_Metrics/Hausdorff_distance_thres.csv\")\n",
    "# Load Haudorff Distance values from 2nd Segmentation Method\n",
    "haus_data_grs = pd.read_csv(\"C:/Users/adeba/Downloads/Project Analysis/Seg_Metrics/Hausdorff_distance_grs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f8b31dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dice Coefficient values from 1st Segmentation Method\n",
    "dice_data_thres = pd.read_csv(\"C:/Users/adeba/Downloads/Project Analysis/Seg_Metrics/Dice_Coefficient_thres.csv\")\n",
    "# Load Dice Coefficient values from 2nd Segmentation Method\n",
    "dice_data_grs = pd.read_csv(\"C:/Users/adeba/Downloads/Project Analysis/Seg_Metrics/Dice_Coefficient_grs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "11adcf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean Dice coefficients for Threshold and GRS Segmentation\n",
    "mean_thres_dice = dice_data_thres[\"Dice coefficient\"].mean()\n",
    "mean_grs_dice = dice_data_grs[\"Dice coefficient\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "babe9b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean True Positives for Threshold and GRS Segmentation\n",
    "mean_thres_pos = dice_data_thres[\"True positives (%)\"].mean()\n",
    "mean_grs_pos = dice_data_grs[\"True positives (%)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "57266b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean True Negatives for Threshold and GRS Segmentation\n",
    "mean_thres_neg = dice_data_thres[\"True negatives (%)\"].mean()\n",
    "mean_grs_neg = dice_data_grs[\"True negatives (%)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "d6308036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean False Positives for Threshold and GRS Segmentation\n",
    "mean_thres_fpos = dice_data_thres[\"False positives (%)\"].mean()\n",
    "mean_grs_fpos = dice_data_grs[\"False positives (%)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f2de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean False Negatives for Threshold and GRS Segmentation\n",
    "mean_thres_fneg = dice_data_thres[\"False negatives (%)\"].mean()\n",
    "mean_grs_fneg = dice_data_grs[\"False negatives (%)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "63ecc75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy\n",
    "accuracy_thres = (mean_thres_pos + mean_thres_neg) / (mean_thres_pos + mean_thres_fpos + mean_thres_neg + mean_thres_fneg)\n",
    "accuracy_grs = (mean_grs_pos + mean_grs_neg) / (mean_grs_pos + mean_grs_fpos + mean_grs_neg + mean_grs_fneg)\n",
    "\n",
    "# Calculate Sensitivity (Recall)\n",
    "sensitivity_thres = mean_thres_pos / (mean_thres_pos + mean_thres_fneg)\n",
    "sensitivity_grs = mean_grs_pos / (mean_grs_pos + mean_grs_fneg)\n",
    "\n",
    "# Calculate Specificity\n",
    "specificity_thres =mean_thres_neg / (mean_thres_neg + mean_thres_fpos)\n",
    "specificity_grs =mean_grs_neg / (mean_grs_neg + mean_grs_fpos)\n",
    "\n",
    "print(\"Threshold Segmentation Method:\")\n",
    "print(\"Accuracy:\", accuracy_thres)\n",
    "print(\"Sensitivity (Recall):\", sensitivity_thres)\n",
    "print(\"Specificity:\", specificity_thres)\n",
    "\n",
    "print(\"\\nGrow from Seeds Segmentation Method:\")\n",
    "print(\"Accuracy:\", accuracy_grs)\n",
    "print(\"Sensitivity (Recall):\", sensitivity_grs)\n",
    "print(\"Specificity:\", specificity_grs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "66d9a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean Maximum Values for Threshold and GRS Segmentation\n",
    "mean_thres_max = haus_data_thres[\"Maximum (mm)\"].mean()\n",
    "mean_grs_max = haus_data_grs[\"Maximum (mm)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "011bab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean Average Values for Threshold and GRS Segmentation\n",
    "mean_thres_avg = haus_data_thres[\"Average (mm)\"].mean()\n",
    "mean_grs_avg = haus_data_grs[\"Average (mm)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "24b94336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the csv file that shows the dice coefficient and hausdorff distance from Comparing 2 Seg. Methods \n",
    "dice = pd.read_csv(\"C:/Users/adeba/Downloads/Project Analysis/Seg_Metrics/Dice_Coefficient .csv\")\n",
    "hausdorff = pd.read_csv(\"C:/Users/adeba/Downloads/Project Analysis/Seg_Metrics/Hausdorff_distance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "3eaefed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean Dice coefficient and Average values for Compared Segmentation Methods\n",
    "mean_dice = dice[\"Dice coefficient\"].mean()\n",
    "mean_avg = hausdorff[\"Average (mm)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "ab7623b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_comb = pd.read_csv(\"C:/users/adeba/Downloads/Project Analysis/Seg_Metrics/Dice_Coefficient_comb.csv\")\n",
    "haus_comb = pd.read_csv(\"C:/Users/adeba/Downloads/Project Analysis/Seg_Metrics/Haus_comb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "5e04e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean Dice coefficient for Segmentation Methods\n",
    "mean_dice_thres = dice_comb[\"Dice_Coeff_Seg_ 1\"].mean()\n",
    "mean_dice_grs = dice_comb[\"Dice_Coeff_Seg_ 2\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "bd9e6f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean Hausdorff  Distance (Average values) for Segmentation Methods\n",
    "mean_haus_thres = haus_comb[\"Haus_Coeff_Seg_ 1\"].mean()\n",
    "mean_haus_gres = haus_comb[\"Haus_Coeff_Seg_ 2\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Define colors for the segmentation methods\n",
    "color_seg1 = \"purple\"\n",
    "color_seg2 = \"orange\"\n",
    "\n",
    "# Select patient IDs 1 to 15\n",
    "top_patient_ids = list(range(1, 16))\n",
    "\n",
    "# Filter the DataFrame for the selected patient IDs\n",
    "filtered_data = haus_comb[haus_comb[\"Patient ID\"].isin(top_patient_ids)]\n",
    "\n",
    "# Increase font size\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Increase marker size and make them bold\n",
    "marker_style = {'markeredgecolor': 'black', 'markersize': 10, 'markeredgewidth': 2}\n",
    "\n",
    "sns.pointplot(data=filtered_data, x=\"Patient ID\", y=\"Haus_Coeff_Seg_ 1\", color=color_seg1, label=\"Threshold\", **marker_style)\n",
    "sns.pointplot(data=filtered_data, x=\"Patient ID\", y=\"Haus_Coeff_Seg_ 2\", color=color_seg2, label=\"Grow from Seed\", **marker_style)\n",
    "\n",
    "plt.xlabel(\"Patient ID\", fontweight='bold')\n",
    "plt.ylabel(\"Average Hausdorff Distance (mm)\", fontweight='bold')\n",
    "plt.title(\"Comparison of Average Hausdorff Distance for Segmentation Methods (Top 15 Patient IDs)\", fontweight='bold', fontsize=16)\n",
    "\n",
    "# Create custom legend with increased and boldened font size\n",
    "custom_legend = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color_seg1, markersize=10, label='Threshold'),\n",
    "                 Line2D([0], [0], marker='o', color='w', markerfacecolor=color_seg2, markersize=10, label='Grow From Seeds')]\n",
    "\n",
    "legend = plt.legend(handles=custom_legend, title=\"Segmentation Method\", loc='upper right', frameon=True)\n",
    "plt.setp(legend.get_title(), fontsize=12, fontweight='bold')  # Set legend title font size and font weight\n",
    "\n",
    "# Bolden specific legend labels\n",
    "legend.get_texts()[0].set_text(\"Threshold\")  # Bolden \"Threshold\"\n",
    "legend.get_texts()[1].set_text(\"Grow from Seeds\")  # Bolden \"Grow from Seeds\"\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
